{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:1024\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_filepath = \"./FX17/Masks/HSI1_Test\"\n",
    "data_filepath = \"./FX17/Data/HSI1_Test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Factor is saing that the image should be divided into 9 sections 3*3 = 9 (kinda like 3 rows 3 columns)\n",
    "import numpy as np\n",
    "def section(img , factor = 3):\n",
    "    secs = []\n",
    "\n",
    "    #This basicaly tests if the image can actually get divided into equal sections\n",
    "    if (img.shape[0] % factor != 0):\n",
    "        return False\n",
    "\n",
    "    #number of pixel in each row and column of the sections\n",
    "    pix_num = int(img.shape[0] / factor)\n",
    "\n",
    "    ptr_x_a = 0\n",
    "    ptr_x_b = pix_num\n",
    "\n",
    "\n",
    "    for i in range(factor):\n",
    "\n",
    "        ptr_y_a = 0\n",
    "        ptr_y_b = pix_num\n",
    "        \n",
    "        for j in range(factor):\n",
    "\n",
    "            secs.append(img[ptr_x_a :ptr_x_b , ptr_y_a : ptr_y_b])\n",
    "            ptr_y_a += pix_num\n",
    "            ptr_y_b += pix_num\n",
    "    \n",
    "        ptr_x_a += pix_num\n",
    "        ptr_x_b += pix_num\n",
    "    \n",
    "    return np.array(secs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "def preprocess(input_data):\n",
    "    data_preprocess = np.zeros_like(input_data)\n",
    "    data_preprocess = savgol_filter(input_data, 15, 2)\n",
    "    data_preprocess = np.gradient(data_preprocess, axis = 2)\n",
    "    data_preprocess = (data_preprocess - np.mean(data_preprocess))/np.std(data_preprocess)\n",
    "    \n",
    "    return data_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing for FX17_HSI1_0003\n",
      "computing for mask FX17_HSI1_0003_mask.png\n",
      "corrected_nparr shape (640, 640, 200)\n",
      "mask shape (640, 640, 3)\n",
      "computing for FX17_HSI1_0036\n",
      "computing for mask FX17_HSI1_0036_mask.png\n",
      "corrected_nparr shape (640, 640, 200)\n",
      "mask shape (640, 640, 3)\n",
      "computing for FX17_HSI1_0038\n",
      "computing for mask FX17_HSI1_0038_mask.png\n",
      "corrected_nparr shape (640, 640, 200)\n",
      "mask shape (640, 640, 3)\n",
      "computing for FX17_HSI1_0026\n",
      "computing for mask FX17_HSI1_0026_mask.png\n",
      "corrected_nparr shape (640, 640, 200)\n",
      "mask shape (640, 640, 3)\n",
      "computing for FX17_HSI1_0014\n",
      "computing for mask FX17_HSI1_0014_mask.png\n",
      "corrected_nparr shape (640, 640, 200)\n",
      "mask shape (640, 640, 3)\n",
      "computing for FX17_HSI1_0029\n",
      "computing for mask FX17_HSI1_0029_mask.png\n",
      "corrected_nparr shape (640, 640, 200)\n",
      "mask shape (640, 640, 3)\n"
     ]
    }
   ],
   "source": [
    "#Pixel level classification\n",
    "from spectral import imshow, view_cube, save_rgb\n",
    "import spectral.io.envi as envi\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def mask2rgb(mask):\n",
    "    rgb = np.zeros(mask.shape+(3,), dtype=np.uint8)\n",
    "    \n",
    "    for i in np.unique(mask):\n",
    "            rgb[mask==i] = LABEL_TO_COLOR[i]\n",
    "            \n",
    "    return rgb\n",
    "\n",
    "def rgb2mask(rgb):\n",
    "    mask = np.zeros((rgb.shape[0], rgb.shape[1]))\n",
    "    for k,v in LABEL_TO_COLOR.items():\n",
    "        mask[np.all(rgb==v, axis=2)] = k\n",
    "        \n",
    "    return mask\n",
    "\n",
    "LABEL_TO_COLOR =  {0:[0,0,0], 1:[255,0,0], 2:[0,255,0], 3:[0,0,255], 4:[255,255,0], 5:[255,0,255], 6:[0,255,255], 7: [255,255,128], 8:[255,128,255], 9:[128,255,255]}\n",
    "\n",
    "filenames = os.listdir(data_filepath)\n",
    "train_hsi_pixel_data = []\n",
    "train_hsi_pixel_label = []\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    filename = filenames[i]\n",
    "    maskname = filename+\"_mask.png\"\n",
    "    print(f\"computing for {filename}\")\n",
    "    print(f\"computing for mask {maskname}\")\n",
    "    dark_ref = envi.open(data_filepath + '/' + filename + '/capture/DARKREF_' + filename + '.hdr', data_filepath + '/' + filename + '/capture/DARKREF_' + filename + '.raw')\n",
    "    white_ref = envi.open(data_filepath + '/' + filename + '/capture/WHITEREF_' + filename + '.hdr', data_filepath + '/' + filename + '/capture/WHITEREF_' + filename + '.raw')\n",
    "    data_ref = envi.open(data_filepath + '/' + filename + '/capture/' + filename + '.hdr', data_filepath + '/' + filename + '/capture/' + filename + '.raw')\n",
    "    \n",
    "    white_nparr = np.array(white_ref.load())\n",
    "    dark_nparr = np.array(dark_ref.load())\n",
    "    data_nparr = np.array(data_ref.load())\n",
    "    corrected_nparr = np.divide(\n",
    "        np.subtract(data_nparr, np.mean(dark_nparr, axis = 0)),\n",
    "        np.subtract(np.mean(white_nparr, axis = 0), np.mean(dark_nparr, axis = 0)))\n",
    "    \n",
    "    if corrected_nparr.shape[0] != 640:\n",
    "        #print(corrected_nparr.shape[0])\n",
    "        corrected_nparr = np.concatenate((corrected_nparr,corrected_nparr[-1].reshape(1,640,224)), axis=0)\n",
    "    \n",
    "    corrected_nparr = preprocess(corrected_nparr[:,:,8:208])\n",
    "    print(f\"corrected_nparr shape {corrected_nparr.shape}\")\n",
    "    \n",
    "    img = Image.open(mask_filepath + \"/\" + maskname)\n",
    "    mask = np.array(img)\n",
    "    print(f\"mask shape {mask.shape}\")\n",
    "    \n",
    "    for i in range(mask.shape[0]):\n",
    "        for j in range(mask.shape[1]):\n",
    "            if mask[i][j][0] == 255 and mask[i][j][1] == 0 and mask[i][j][2] == 0:\n",
    "                train_hsi_pixel_data.append(corrected_nparr[i][j])\n",
    "                train_hsi_pixel_label.append(np.eye(9)[0])\n",
    "            elif mask[i][j][0] == 0 and mask[i][j][1] == 255 and mask[i][j][2] == 0:\n",
    "                train_hsi_pixel_data.append(corrected_nparr[i][j])\n",
    "                train_hsi_pixel_label.append(np.eye(9)[1])\n",
    "            elif mask[i][j][0] == 0 and mask[i][j][1] == 0 and mask[i][j][2] == 255:\n",
    "                train_hsi_pixel_data.append(corrected_nparr[i][j])\n",
    "                train_hsi_pixel_label.append(np.eye(9)[2])\n",
    "            elif mask[i][j][0] == 255 and mask[i][j][1] == 255 and mask[i][j][2] == 0:\n",
    "                train_hsi_pixel_data.append(corrected_nparr[i][j])\n",
    "                train_hsi_pixel_label.append(np.eye(9)[3])\n",
    "            elif mask[i][j][0] == 255 and mask[i][j][1] == 0 and mask[i][j][2] == 255:\n",
    "                train_hsi_pixel_data.append(corrected_nparr[i][j])\n",
    "                train_hsi_pixel_label.append(np.eye(9)[4])\n",
    "            elif mask[i][j][0] == 0 and mask[i][j][1] == 255 and mask[i][j][2] == 255:\n",
    "                train_hsi_pixel_data.append(corrected_nparr[i][j])\n",
    "                train_hsi_pixel_label.append(np.eye(9)[5])\n",
    "            elif mask[i][j][0] == 255 and mask[i][j][1] == 255 and mask[i][j][2] == 128:\n",
    "                train_hsi_pixel_data.append(corrected_nparr[i][j])\n",
    "                train_hsi_pixel_label.append(np.eye(9)[6])\n",
    "            elif mask[i][j][0] == 255 and mask[i][j][1] == 128 and mask[i][j][2] == 255:\n",
    "                train_hsi_pixel_data.append(corrected_nparr[i][j])\n",
    "                train_hsi_pixel_label.append(np.eye(9)[7])\n",
    "            elif mask[i][j][0] == 128 and mask[i][j][1] == 255 and mask[i][j][2] == 255:\n",
    "                train_hsi_pixel_data.append(corrected_nparr[i][j])\n",
    "                train_hsi_pixel_label.append(np.eye(9)[8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "607555"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_hsi_pixel_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For both train and test the name is train_hsi_pixel_data\n",
    "test_pixels = np.array(train_hsi_pixel_data)\n",
    "test_labels = np.array(train_hsi_pixel_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(607555, 9)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(607555, 200)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pixels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class makeDataset(Dataset):\n",
    "    def __init__(self, X, Y): \n",
    "        self.x = X\n",
    "        self.y = Y\n",
    "        #self.i = index\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        feature = self.x[index, :]\n",
    "        label = self.y[index, :]\n",
    "        #i = self.i[index]\n",
    "        \n",
    "        feature = torch.tensor(feature, dtype=torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        return {\n",
    "            'features': feature,\n",
    "            'labels' : label,\n",
    "            #'index' : i\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = makeDataset(test_pixels, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pixel Level Classification\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DeepSpectra(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(DeepSpectra, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.pre = pre_block(sampling_point = 200)\n",
    "        self.conv1 = conv_block(\n",
    "            in_channels=1,\n",
    "            out_channels=8,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding = 1\n",
    "        )\n",
    "        self.conv2 = nn.Conv1d(8, 16, 3, 1, 1)\n",
    "\n",
    "        # In this order: in_channels, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, out_1x1pool\n",
    "        self.inception1 = Naive_inception_block(16, 8, 8, 8, 8)\n",
    "        #self.inception2 = Naive_inception_block(32, 16, 16, 16, 16)\n",
    "        self.fc1 = nn.Linear(50*32, 128)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.fc2 = nn.Linear(128, self.num_classes)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pre(x)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        x = self.inception1(x)\n",
    "        #x = self.inception2(x)\n",
    "        x = x.view(-1, 50*32)\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x)\n",
    "\n",
    "\n",
    "class Naive_inception_block(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, out_1x1, out_3x3, out_5x5, out_1x1pool):\n",
    "        super(Naive_inception_block, self).__init__()\n",
    "        self.branch1 = conv_block(in_channels, out_1x1, kernel_size=1)\n",
    "\n",
    "        self.branch2 = conv_block(in_channels, out_3x3, kernel_size=3, padding=1)\n",
    "\n",
    "        self.branch3 = conv_block(in_channels, out_5x5, kernel_size=5, padding=2)\n",
    "        \n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1, padding=1),\n",
    "            conv_block(in_channels, out_1x1pool, kernel_size=1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.cat(\n",
    "            [self.branch1(x), self.branch2(x), self.branch3(x), self.branch4(x)], 1\n",
    "        )\n",
    "        \n",
    "    \n",
    "class Inception_block(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, out_1x1pool\n",
    "    ):\n",
    "        super(Inception_block, self).__init__()\n",
    "        self.branch1 = conv_block(in_channels, out_1x1, kernel_size=1)\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "            conv_block(in_channels, red_3x3, kernel_size=1),\n",
    "            conv_block(red_3x3, out_3x3, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "        self.branch3 = nn.Sequential(\n",
    "            conv_block(in_channels, red_5x5, kernel_size=1),\n",
    "            conv_block(red_5x5, out_5x5, kernel_size=5, padding=2),\n",
    "        )\n",
    "\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1, padding=1),\n",
    "            conv_block(in_channels, out_1x1pool, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat(\n",
    "            [self.branch1(x), self.branch2(x), self.branch3(x), self.branch4(x)], 1\n",
    "        )\n",
    "\n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(conv_block, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, **kwargs)\n",
    "        self.batchnorm = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.conv(x))\n",
    "    \n",
    "class pre_block(nn.Module):\n",
    "    def __init__(self, sampling_point):\n",
    "        super().__init__()\n",
    "        self.pool1 = nn.AvgPool1d(kernel_size = 5, stride = 1, padding = 2)\n",
    "        self.pool2 = nn.AvgPool1d(kernel_size = 13, stride = 1, padding = 6)\n",
    "        self.pool3 = nn.AvgPool1d(kernel_size = 7, stride = 1, padding = 3)\n",
    "        self.ln = nn.LayerNorm(sampling_point)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.ln(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on a GPU\n"
     ]
    }
   ],
   "source": [
    "#Pixel Level Classification\n",
    "from tqdm import tqdm\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Running on a GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on a CPU\")\n",
    "    \n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Conv1d):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "        \n",
    "def train(model, dataloader, optimizer, loss_fn):\n",
    "    print(\"training started\")\n",
    "    model.train()\n",
    "    counter = 0\n",
    "    train_running_loss = 0.0\n",
    "    train_running_acc=0.0\n",
    "    for i, data in tqdm(enumerate(dataloader)):\n",
    "        counter += 1\n",
    "        # print(f\"class type of input data {type(data)}\")\n",
    "        # extract the features and labels\n",
    "        features = data['features'].view(-1,1,200).to(device)\n",
    "        # print(features.size())\n",
    "        labels = data['labels'].to(device)\n",
    "        \n",
    "        # zero-out the optimizer gradients\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        #outputs = model(features, labels, infer = False)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        matches = [torch.argmax(i) == torch.argmax(j) for i, j in zip(outputs,labels)]\n",
    "        acc = matches.count(True)/len(matches)\n",
    "        train_running_loss += loss.item()\n",
    "        train_running_acc += acc\n",
    "        \n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        # update optimizer parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss = train_running_loss / counter\n",
    "    train_acc = train_running_acc / counter   \n",
    "    print(train_acc) \n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    counter = 0\n",
    "    running_loss = 0.0\n",
    "    running_acc=0.0\n",
    "    matches = []\n",
    "    mislabel =[]\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(dataloader):\n",
    "            counter += 1\n",
    "            # extract the features and labels\n",
    "            features = data['features'].view(-1,1,200).to(device)\n",
    "            labels = data['labels'].to(device)\n",
    "            #index = data['index'].to(device)\n",
    "            outputs = model(features)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            matches = [torch.argmax(i) == torch.argmax(j) for i, j in zip(outputs,labels)]\n",
    "            acc = matches.count(True)/len(matches)\n",
    "            running_acc += acc\n",
    "            \n",
    "    loss = running_loss / counter\n",
    "    accuracy = running_acc / counter  \n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1250799/4223536864.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n",
      "4747it [01:08, 69.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9657974773541184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "num_class = 9\n",
    "epochs = 2\n",
    "model = DeepSpectra(num_class).to(device)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "train_loss, train_acc = train(model, test_loader, optimizer, loss_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyperspectral",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
